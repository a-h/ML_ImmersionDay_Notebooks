{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import AWS Sagemaker package and get IAM role\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update package manager because Sagemaker notebook instances look out of data\n",
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrade scikit-learn as imblearn has some recent changes\n",
    "!pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install python package for doing SMOTE - because we have massive class imbalance\n",
    "!pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install matplotlib for visualizing tree later\n",
    "!pip install -U matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install graphviz as matplotlib will need this for the tree layout \n",
    "!pip install -U graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install xgboost so we can inspect, locally, the model objects we build\n",
    "!pip install -U xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary numerical libraries\n",
    "import numpy as np                                \n",
    "import pandas as pd                               \n",
    "import matplotlib.pyplot as plt   \n",
    "from IPython.display import Image                 \n",
    "from IPython.display import display               \n",
    "from sklearn.datasets import dump_svmlight_file   \n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import gmtime, strftime                 \n",
    "import sys                                        \n",
    "import math                                       \n",
    "import json\n",
    "import boto3\n",
    "\n",
    "import s3fs\n",
    "import pickle\n",
    "import tarfile\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import imblearn package for doing SMOTE resampling\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy raw data from S3 bucket to filepath accessible to the notebook\n",
    "data_bucket = 'immersion-day-ccdata'\n",
    "raw_data_filename = 'creditcard.csv'\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(data_bucket).download_file(raw_data_filename, 'ccRaw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into pandas dataframe\n",
    "ccRaw = pd.read_csv('./ccRaw.csv')\n",
    "pd.set_option('display.max_rows', 20) \n",
    "ccRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at response variable. Looks like we don't have to convert response variable to a dummy variable - already done\n",
    "ccRaw.Class.unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create dataset splits for training, validation and test\n",
    "# Split out test data first (10% of the total)\n",
    "nonTest_data, test_data = train_test_split( ccRaw, test_size=0.1, random_state=42, stratify=ccRaw['Class'].array )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SMOTE on nonTest_data to synthetically generate a more balanced data set\n",
    "nonTest_X = nonTest_data.drop('Class', axis=1)\n",
    "nonTest_y = nonTest_data['Class']\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "nonTest_X_res, nonTest_y_res = smote.fit_resample(nonTest_X, nonTest_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split resampled non-test data into training and validation sets\n",
    "train_data_X, validation_data_X, train_data_y, validation_data_y = train_test_split( nonTest_X_res, nonTest_y_res, \\\n",
    "                                                                                    test_size=(2.0/7.0), random_state=1987, stratify=nonTest_y_res ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform training, validation, and test sets into libSVM format for use in xgboost\n",
    "dump_svmlight_file(X=train_data_X, y=train_data_y, f='train.libsvm')\n",
    "dump_svmlight_file(X=validation_data_X, y=validation_data_y, f='validation.libsvm')\n",
    "dump_svmlight_file(X=test_data.drop(['Class'], axis=1), y=test_data['Class'], f='test.libsvm')\n",
    "\n",
    "# Write training and validation sets to our S3 bucket\n",
    "bucket = '<your_s3_bucket_name_here>'\n",
    "prefix = 'sagemaker/ccData_xgboost'\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(prefix + '/train/train.libsvm').upload_file('train.libsvm')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(prefix + '/validation/validation.libsvm').upload_file('validation.libsvm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify containers defining training instances with xgboost algorithm\n",
    "containers = {\n",
    "                'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',\n",
    "                'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',\n",
    "                'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest',\n",
    "                'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest'\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get AWS sagemaker client\n",
    "sm = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define HyperParameterTuningJob\n",
    "# We will only tune the learning rate by maximizing the AUC value of the \n",
    "# validation set. The hyperparameter search is a random one, using a sample of\n",
    "# 10 training jobs - better methods for searching the hyperparameter space are \n",
    "# available, but for simplicty and demonstration purposes we will use the \n",
    "# random search method. Run a max of 3 training jobs in parallel\n",
    "job_name = \"xgb-cc\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "response = sm.create_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=job_name,\n",
    "    HyperParameterTuningJobConfig={\n",
    "        'Strategy': 'Random',\n",
    "        'HyperParameterTuningJobObjective': {\n",
    "            'Type': 'Maximize',\n",
    "            'MetricName': 'validation:auc'\n",
    "        },\n",
    "        'ResourceLimits': {\n",
    "            'MaxNumberOfTrainingJobs': 10,\n",
    "            'MaxParallelTrainingJobs': 3\n",
    "        },\n",
    "        'ParameterRanges': {\n",
    "            'ContinuousParameterRanges': [\n",
    "                {\n",
    "                    'Name': 'eta',\n",
    "                    'MinValue': '0.01',\n",
    "                    'MaxValue': '0.4',\n",
    "                    'ScalingType': 'Linear'\n",
    "                },\n",
    "            ]\n",
    "        },\n",
    "        'TrainingJobEarlyStoppingType': 'Off'\n",
    "    },\n",
    "    TrainingJobDefinition={\n",
    "        'StaticHyperParameters': {\n",
    "            \"max_depth\":\"3\",\n",
    "            \"eval_metric\":\"auc\",\n",
    "            \"scale_pos_weight\":\"2.0\",\n",
    "            \"subsample\":\"0.5\",\n",
    "            \"objective\":\"binary:logistic\",\n",
    "            \"num_round\":\"100\",\n",
    "            \"seed\":\"42\"\n",
    "        },\n",
    "        'AlgorithmSpecification': {\n",
    "        'TrainingImage': containers[boto3.Session().region_name],\n",
    "        'TrainingInputMode': \"File\"\n",
    "        },\n",
    "        'RoleArn': role,\n",
    "        \"InputDataConfig\": [\n",
    "            {\n",
    "                \"ChannelName\": \"train\",\n",
    "                \"DataSource\": {\n",
    "                    \"S3DataSource\": {\n",
    "                        \"S3DataType\": \"S3Prefix\",\n",
    "                        \"S3Uri\": \"s3://{}/{}/train\".format(bucket, prefix),\n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                    }\n",
    "                },\n",
    "                \"ContentType\": \"libsvm\",\n",
    "                \"CompressionType\": \"None\"\n",
    "            },\n",
    "            {\n",
    "                \"ChannelName\": \"validation\",\n",
    "                \"DataSource\": {\n",
    "                    \"S3DataSource\": {\n",
    "                        \"S3DataType\": \"S3Prefix\",\n",
    "                        \"S3Uri\": \"s3://{}/{}/validation\".format(bucket, prefix),\n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                    }\n",
    "                },\n",
    "                \"ContentType\": \"libsvm\",\n",
    "                \"CompressionType\": \"None\"\n",
    "            }\n",
    "        ],\n",
    "        \"OutputDataConfig\": {\n",
    "            \"S3OutputPath\": \"s3://{}/{}/xgboost-ccData-hyperopt/output\".format(bucket, prefix)\n",
    "        },\n",
    "        'ResourceConfig': {\n",
    "        'InstanceCount': 1,\n",
    "        'InstanceType': 'ml.c4.xlarge',\n",
    "        'VolumeSizeInGB': 10\n",
    "        },\n",
    "        'StoppingCondition': {\n",
    "            'MaxRuntimeInSeconds': 60 * 60\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get status of HyperParameterTuningJob to see that it has completed\n",
    "status_hyperopt = sm.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=job_name)['HyperParameterTuningJobStatus']\n",
    "\n",
    "status_hyperopt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now inspect the results from the different training jobs. We want to select\n",
    "# the training job that gave the highest AUC value on the validation set\n",
    "\n",
    "# this gets the results from all the training jobs as a pandas dataframe\n",
    "df_hyperopt = sagemaker.HyperparameterTuningJobAnalytics(job_name).dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the training job results\n",
    "df_hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the details of the 'best' training job\n",
    "df_hyperopt.loc[df_hyperopt['FinalObjectiveValue'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture name of the best training job\n",
    "optimalTrainingJob = df_hyperopt.loc[df_hyperopt['FinalObjectiveValue'].idxmax()]['TrainingJobName']\n",
    "optimalTrainingJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in pickled model object for the optimal training job\n",
    "# so we can inspect the model locally\n",
    "model_path_prefix = 's3://david-hoyle-sagemaker/sagemaker/ccData_xgboost/xgboost-ccData-hyperopt/output/' \n",
    "model_path = model_path_prefix + optimalTrainingJob + '/output/model.tar.gz'\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "with fs.open(model_path, 'rb') as f:\n",
    "    with tarfile.open(fileobj=f, mode='r') as tar_f:\n",
    "        with tar_f.extractfile('xgboost-model') as extracted_f:\n",
    "            xgb_optimalModel = pickle.load(extracted_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first tree in the ensemble of the optimal training job\n",
    "xgb.plot_tree(xgb_optimalModel,num_trees=0)\n",
    "plt.rcParams['figure.figsize'] = [50, 100]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot second tree in the ensemble of the optimal training job\n",
    "xgb.plot_tree(xgb_optimalModel,num_trees=1)\n",
    "plt.rcParams['figure.figsize'] = [50, 100]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model for use in endpoint that we will use for predictions\n",
    "inference_job_name = 'optimal-cc-xgb' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=inference_job_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        'Image': containers[boto3.Session().region_name],\n",
    "        'ModelDataUrl': sm.describe_training_job(TrainingJobName=optimalTrainingJob)['ModelArtifacts']['S3ModelArtifacts']})\n",
    "\n",
    "print(create_model_response['ModelArn'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create endpoint config\n",
    "xgboost_endpoint_config = 'ccData-xgboost-endpoint-config-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(xgboost_endpoint_config)\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=xgboost_endpoint_config,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': 'ml.t2.medium',\n",
    "        'InitialInstanceCount': 1,\n",
    "        'ModelName': inference_job_name,\n",
    "        'VariantName': 'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create endpoint\n",
    "xgboost_endpoint = 'EXAMPLE-ccData-xgb-endpoint-' + strftime(\"%Y%m%d%H%M\", gmtime())\n",
    "print(xgboost_endpoint)\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=xgboost_endpoint,\n",
    "    EndpointConfigName=xgboost_endpoint_config)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sm.describe_endpoint(EndpointName=xgboost_endpoint)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "try:\n",
    "    sm.get_waiter('endpoint_in_service').wait(EndpointName=xgboost_endpoint)\n",
    "finally:\n",
    "    resp = sm.describe_endpoint(EndpointName=xgboost_endpoint)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Arn: \" + resp['EndpointArn'])\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "    if status != 'InService':\n",
    "        message = sm.describe_endpoint(EndpointName=xgboost_endpoint)['FailureReason']\n",
    "        print('Endpoint creation failed with the following error: {}'.format(message))\n",
    "        raise Exception('Endpoint creation did not succeed')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = boto3.client('runtime.sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for calling endpoint with input test data\n",
    "def do_predict(data, endpoint_name, content_type):\n",
    "    payload = '\\n'.join(data)\n",
    "    response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType=content_type, \n",
    "                                   Body=payload)\n",
    "    result = response['Body'].read()\n",
    "    result = result.decode(\"utf-8\")\n",
    "    result = result.split(',')\n",
    "    preds = [float((num)) for num in result]\n",
    "    preds = [round(num) for num in preds]\n",
    "    return preds\n",
    "\n",
    "def batch_predict(data, batch_size, endpoint_name, content_type):\n",
    "    items = len(data)\n",
    "    arrs = []\n",
    "    \n",
    "    for offset in range(0, items, batch_size):\n",
    "        if offset+batch_size < items:\n",
    "            results = do_predict(data[offset:(offset+batch_size)], endpoint_name, content_type)\n",
    "            arrs.extend(results)\n",
    "        else:\n",
    "            arrs.extend(do_predict(data[offset:items], endpoint_name, content_type))\n",
    "        sys.stdout.write('.')\n",
    "    return(arrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Calculate error rate for our test set\n",
    "import json\n",
    "\n",
    "with open('test.libsvm', 'r') as f:\n",
    "    payload = f.read().strip()\n",
    "\n",
    "labels = [int(line.split(' ')[0]) for line in payload.split('\\n')]\n",
    "test_data = [line for line in payload.split('\\n')]\n",
    "preds = batch_predict(test_data, 100, xgboost_endpoint, 'text/x-libsvm')\n",
    "\n",
    "print ('\\nerror rate=%f' % ( sum(1 for i in range(len(preds)) if preds[i]!=labels[i]) /float(len(preds))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "pd.crosstab(index=np.array(labels), columns=np.array(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete endpoint\n",
    "sm.delete_endpoint(EndpointName=xgboost_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
